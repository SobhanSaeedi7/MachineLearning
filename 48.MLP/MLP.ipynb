{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "v1xCs-bEUNfe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTDoqFmcVXfb",
        "outputId": "acb8638f-e516-4858-fd6a-ab71031d4ceb"
      },
      "outputs": [],
      "source": [
        "dataset = load_digits()\n",
        "X = dataset.data\n",
        "Y = dataset.target\n",
        "Y = np.eye(10)[Y] # one hot\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "FEZTVKb6f3jI"
      },
      "outputs": [],
      "source": [
        "def sigmoid(X):\n",
        "  return 1 / (1 + np.exp(-X))\n",
        "\n",
        "def softmax(X):\n",
        "  return np.exp(X) / np.sum(np.exp(X))\n",
        "\n",
        "def root_mean_squired_error(Y_gt, Y_pred):\n",
        "  return np.sqrt(np.mean((Y_gt - Y_pred) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hhKH0s5IiD91"
      },
      "outputs": [],
      "source": [
        "epochs = 80\n",
        "Î· = 0.001 # learning rate\n",
        "\n",
        "D_in = X_train.shape[1]\n",
        "H1 = 128\n",
        "H2 = 32\n",
        "D_out = Y_train.shape[1]\n",
        "# D_out = len(np.unique(Y_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "u9lai2urps8W"
      },
      "outputs": [],
      "source": [
        "W1 = np.random.randn(D_in, H1)\n",
        "W2 = np.random.randn(H1, H2)\n",
        "W3 = np.random.randn(H2, D_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "nJWcekJZqLvJ"
      },
      "outputs": [],
      "source": [
        "B1 = np.random.randn(1, H1)\n",
        "B2 = np.random.randn(1, H2)\n",
        "B3 = np.random.randn(1, D_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGXD1SyBqqX4",
        "outputId": "e36d7fb7-b367-4643-8012-1019364edc6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss train :  0.26315796733977403\n",
            "accuracy train :  0.4627696590118302\n",
            "loss test :  0.2626399346018338\n",
            "accuracy test :  0.4638888888888889\n",
            "loss train :  0.24246002919036513\n",
            "accuracy train :  0.5706332637439109\n",
            "loss test :  0.24673761955597698\n",
            "accuracy test :  0.5361111111111111\n",
            "loss train :  0.2261768341963234\n",
            "accuracy train :  0.6416144745998609\n",
            "loss test :  0.23426677298917026\n",
            "accuracy test :  0.5944444444444444\n",
            "loss train :  0.21265645121417592\n",
            "accuracy train :  0.6917188587334725\n",
            "loss test :  0.22299836061956188\n",
            "accuracy test :  0.6416666666666667\n",
            "loss train :  0.20076073445555703\n",
            "accuracy train :  0.7432150313152401\n",
            "loss test :  0.21385487047381369\n",
            "accuracy test :  0.6916666666666667\n",
            "loss train :  0.1906362234809665\n",
            "accuracy train :  0.7696590118302018\n",
            "loss test :  0.20684277932481288\n",
            "accuracy test :  0.7138888888888889\n",
            "loss train :  0.18197722094549668\n",
            "accuracy train :  0.7974947807933194\n",
            "loss test :  0.2007033681922527\n",
            "accuracy test :  0.7305555555555555\n",
            "loss train :  0.1741822105396452\n",
            "accuracy train :  0.8183716075156576\n",
            "loss test :  0.19544817626097588\n",
            "accuracy test :  0.7388888888888889\n",
            "loss train :  0.16711574639189303\n",
            "accuracy train :  0.8329853862212944\n",
            "loss test :  0.19179014632914215\n",
            "accuracy test :  0.7472222222222222\n",
            "loss train :  0.16099712562449317\n",
            "accuracy train :  0.8448155880306193\n",
            "loss test :  0.18746164445276073\n",
            "accuracy test :  0.7527777777777778\n",
            "loss train :  0.15534067938698115\n",
            "accuracy train :  0.8622129436325678\n",
            "loss test :  0.18361386669687185\n",
            "accuracy test :  0.7638888888888888\n",
            "loss train :  0.15025032525502335\n",
            "accuracy train :  0.8712595685455811\n",
            "loss test :  0.18053168215462675\n",
            "accuracy test :  0.7777777777777778\n",
            "loss train :  0.14554932822962668\n",
            "accuracy train :  0.8768267223382046\n",
            "loss test :  0.17765065669566016\n",
            "accuracy test :  0.7777777777777778\n",
            "loss train :  0.14112061172349322\n",
            "accuracy train :  0.8872651356993737\n",
            "loss test :  0.1751405977117686\n",
            "accuracy test :  0.7805555555555556\n",
            "loss train :  0.13695188379141696\n",
            "accuracy train :  0.894919972164231\n",
            "loss test :  0.1727850489834815\n",
            "accuracy test :  0.7861111111111111\n",
            "loss train :  0.13305470857405585\n",
            "accuracy train :  0.906054279749478\n",
            "loss test :  0.1706097819887401\n",
            "accuracy test :  0.7916666666666666\n",
            "loss train :  0.1294108290965641\n",
            "accuracy train :  0.9137091162143354\n",
            "loss test :  0.1687300535562753\n",
            "accuracy test :  0.7972222222222223\n",
            "loss train :  0.126001191729845\n",
            "accuracy train :  0.9220598469032707\n",
            "loss test :  0.16711220650993025\n",
            "accuracy test :  0.8\n",
            "loss train :  0.12283776927257976\n",
            "accuracy train :  0.9241475295755045\n",
            "loss test :  0.16569070390574384\n",
            "accuracy test :  0.8\n",
            "loss train :  0.11993786465462113\n",
            "accuracy train :  0.9262352122477383\n",
            "loss test :  0.1643758736320403\n",
            "accuracy test :  0.8027777777777778\n",
            "loss train :  0.11725705280157471\n",
            "accuracy train :  0.929714683368128\n",
            "loss test :  0.1631050742720886\n",
            "accuracy test :  0.8083333333333333\n",
            "loss train :  0.11475058638898632\n",
            "accuracy train :  0.9352818371607515\n",
            "loss test :  0.16187774440470976\n",
            "accuracy test :  0.8138888888888889\n",
            "loss train :  0.11236279275795442\n",
            "accuracy train :  0.9387613082811412\n",
            "loss test :  0.16073344991929844\n",
            "accuracy test :  0.8222222222222222\n",
            "loss train :  0.10998668008389793\n",
            "accuracy train :  0.941544885177453\n",
            "loss test :  0.15961315628658582\n",
            "accuracy test :  0.8277777777777777\n",
            "loss train :  0.10772290831036256\n",
            "accuracy train :  0.942936673625609\n",
            "loss test :  0.15857535814125487\n",
            "accuracy test :  0.8277777777777777\n",
            "loss train :  0.10551324924432119\n",
            "accuracy train :  0.9457202505219207\n",
            "loss test :  0.1576821420970649\n",
            "accuracy test :  0.8305555555555556\n",
            "loss train :  0.10347255956659777\n",
            "accuracy train :  0.9491997216423104\n",
            "loss test :  0.15687044045102305\n",
            "accuracy test :  0.8305555555555556\n",
            "loss train :  0.10156209001663323\n",
            "accuracy train :  0.9498956158663883\n",
            "loss test :  0.1560944918729288\n",
            "accuracy test :  0.8305555555555556\n",
            "loss train :  0.09974953309178153\n",
            "accuracy train :  0.9526791927627001\n",
            "loss test :  0.1553501406139419\n",
            "accuracy test :  0.8277777777777777\n",
            "loss train :  0.09802061533683748\n",
            "accuracy train :  0.953375086986778\n",
            "loss test :  0.1546900759698459\n",
            "accuracy test :  0.8277777777777777\n",
            "loss train :  0.09636174068376521\n",
            "accuracy train :  0.9554627696590118\n",
            "loss test :  0.1541628997508635\n",
            "accuracy test :  0.8277777777777777\n",
            "loss train :  0.09470290290314631\n",
            "accuracy train :  0.9561586638830898\n",
            "loss test :  0.15365070522859237\n",
            "accuracy test :  0.8305555555555556\n",
            "loss train :  0.0931490651429553\n",
            "accuracy train :  0.9568545581071677\n",
            "loss test :  0.15310333717539745\n",
            "accuracy test :  0.8305555555555556\n",
            "loss train :  0.09170163398178025\n",
            "accuracy train :  0.9575504523312457\n",
            "loss test :  0.15255851142341495\n",
            "accuracy test :  0.8333333333333334\n",
            "loss train :  0.09032124911672347\n",
            "accuracy train :  0.9589422407794015\n",
            "loss test :  0.15200572656936698\n",
            "accuracy test :  0.8388888888888889\n",
            "loss train :  0.08898438408111518\n",
            "accuracy train :  0.9596381350034795\n",
            "loss test :  0.1514208411805119\n",
            "accuracy test :  0.8416666666666667\n",
            "loss train :  0.08765202377160798\n",
            "accuracy train :  0.9610299234516354\n",
            "loss test :  0.15079217310823353\n",
            "accuracy test :  0.8416666666666667\n",
            "loss train :  0.08629125069357287\n",
            "accuracy train :  0.9624217118997912\n",
            "loss test :  0.15022275602892182\n",
            "accuracy test :  0.8444444444444444\n",
            "loss train :  0.0849978961013273\n",
            "accuracy train :  0.9624217118997912\n",
            "loss test :  0.14971835123245955\n",
            "accuracy test :  0.8444444444444444\n",
            "loss train :  0.08376518685531814\n",
            "accuracy train :  0.9624217118997912\n",
            "loss test :  0.1492272378840468\n",
            "accuracy test :  0.85\n",
            "loss train :  0.08258428340025946\n",
            "accuracy train :  0.9631176061238692\n",
            "loss test :  0.1487288202258309\n",
            "accuracy test :  0.8527777777777777\n",
            "loss train :  0.08144889214205141\n",
            "accuracy train :  0.965205288796103\n",
            "loss test :  0.14821113355071733\n",
            "accuracy test :  0.8527777777777777\n",
            "loss train :  0.0803478467422964\n",
            "accuracy train :  0.9665970772442589\n",
            "loss test :  0.14768749470990836\n",
            "accuracy test :  0.8527777777777777\n",
            "loss train :  0.07928318747132788\n",
            "accuracy train :  0.9665970772442589\n",
            "loss test :  0.14719057334102145\n",
            "accuracy test :  0.8527777777777777\n",
            "loss train :  0.07826262378320119\n",
            "accuracy train :  0.9686847599164927\n",
            "loss test :  0.14672330126083372\n",
            "accuracy test :  0.8555555555555555\n",
            "loss train :  0.07728499355247262\n",
            "accuracy train :  0.9700765483646486\n",
            "loss test :  0.14627012755729327\n",
            "accuracy test :  0.8555555555555555\n",
            "loss train :  0.0763428250896117\n",
            "accuracy train :  0.9707724425887265\n",
            "loss test :  0.14582001968115527\n",
            "accuracy test :  0.8555555555555555\n",
            "loss train :  0.07542968231851596\n",
            "accuracy train :  0.9728601252609603\n",
            "loss test :  0.14536332397396165\n",
            "accuracy test :  0.8555555555555555\n",
            "loss train :  0.07454229895015611\n",
            "accuracy train :  0.9749478079331941\n",
            "loss test :  0.1448908659709953\n",
            "accuracy test :  0.8555555555555555\n",
            "loss train :  0.07367523141526079\n",
            "accuracy train :  0.97633959638135\n",
            "loss test :  0.14439771303495813\n",
            "accuracy test :  0.8555555555555555\n",
            "loss train :  0.07281321915469315\n",
            "accuracy train :  0.97633959638135\n",
            "loss test :  0.14390491726495563\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.07190705688639006\n",
            "accuracy train :  0.9770354906054279\n",
            "loss test :  0.14351124397831766\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.07095149664145059\n",
            "accuracy train :  0.977731384829506\n",
            "loss test :  0.14315387590369846\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.0701061176520503\n",
            "accuracy train :  0.977731384829506\n",
            "loss test :  0.14280946357474764\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.06930238293146938\n",
            "accuracy train :  0.9784272790535838\n",
            "loss test :  0.1424941261213484\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.06852884055630841\n",
            "accuracy train :  0.9784272790535838\n",
            "loss test :  0.14219491145879706\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.06778295618351114\n",
            "accuracy train :  0.9791231732776617\n",
            "loss test :  0.14190803753513398\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.06705953263546163\n",
            "accuracy train :  0.9791231732776617\n",
            "loss test :  0.14163311415416657\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.06635399348534345\n",
            "accuracy train :  0.9791231732776617\n",
            "loss test :  0.14137112057803297\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.06566465819773706\n",
            "accuracy train :  0.9798190675017397\n",
            "loss test :  0.14112316998341337\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.06499168522982711\n",
            "accuracy train :  0.9798190675017397\n",
            "loss test :  0.14088795027008702\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.06433493011262655\n",
            "accuracy train :  0.9805149617258176\n",
            "loss test :  0.14066205104105356\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.06369310471311918\n",
            "accuracy train :  0.9805149617258176\n",
            "loss test :  0.1404428851129145\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.06306439441838464\n",
            "accuracy train :  0.9812108559498957\n",
            "loss test :  0.1402297245611639\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.06244783004348252\n",
            "accuracy train :  0.9819067501739736\n",
            "loss test :  0.14002307521743615\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.06184329861013051\n",
            "accuracy train :  0.9819067501739736\n",
            "loss test :  0.13982406162923694\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.06125041195926828\n",
            "accuracy train :  0.9826026443980515\n",
            "loss test :  0.13963420345222577\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.06066773013429959\n",
            "accuracy train :  0.9826026443980515\n",
            "loss test :  0.13945513619580996\n",
            "accuracy test :  0.8583333333333333\n",
            "loss train :  0.06009295701573107\n",
            "accuracy train :  0.9826026443980515\n",
            "loss test :  0.13928810013807302\n",
            "accuracy test :  0.8583333333333333\n",
            "loss train :  0.05952416675312885\n",
            "accuracy train :  0.9832985386221295\n",
            "loss test :  0.13913326365393075\n",
            "accuracy test :  0.8583333333333333\n",
            "loss train :  0.05896228268131727\n",
            "accuracy train :  0.9832985386221295\n",
            "loss test :  0.1389905490447569\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.05841102062545889\n",
            "accuracy train :  0.9832985386221295\n",
            "loss test :  0.13886285089726844\n",
            "accuracy test :  0.8611111111111112\n",
            "loss train :  0.057871380472836687\n",
            "accuracy train :  0.9846903270702854\n",
            "loss test :  0.13875286944585147\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.05734128375448942\n",
            "accuracy train :  0.9846903270702854\n",
            "loss test :  0.13865553452171078\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.05681832238543224\n",
            "accuracy train :  0.9853862212943633\n",
            "loss test :  0.13855958785469047\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.05630052537485219\n",
            "accuracy train :  0.9860821155184412\n",
            "loss test :  0.13845387901171322\n",
            "accuracy test :  0.8638888888888889\n",
            "loss train :  0.055786475297720864\n",
            "accuracy train :  0.9860821155184412\n",
            "loss test :  0.13833194640558125\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.055275195020423114\n",
            "accuracy train :  0.9867780097425192\n",
            "loss test :  0.13819197563351532\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.05476594868067712\n",
            "accuracy train :  0.9874739039665971\n",
            "loss test :  0.13803411328513082\n",
            "accuracy test :  0.8666666666666667\n",
            "loss train :  0.054258149064266514\n",
            "accuracy train :  0.9874739039665971\n",
            "loss test :  0.13785983086015416\n",
            "accuracy test :  0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  Y_pred_train = []\n",
        "\n",
        "  # train\n",
        "\n",
        "  for x, y in zip(X_train, Y_train):\n",
        "\n",
        "      x = x.reshape(-1, 1)\n",
        "\n",
        "      # forward\n",
        "\n",
        "      # layer 1\n",
        "      out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "      # layer 2\n",
        "      out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "      # layer 3\n",
        "      y_pred = softmax(out2 @ W3 + B3)\n",
        "\n",
        "      Y_pred_train.append(y_pred)\n",
        "\n",
        "      # backward\n",
        "\n",
        "      # layer 3\n",
        "      erorr = -2 * (y - y_pred)\n",
        "      grad_B3 = erorr\n",
        "      grad_W3 = out2.T @ erorr\n",
        "\n",
        "      # layer 2\n",
        "      erorr = erorr @ W3.T * out2 * (1 - out2)\n",
        "      grad_B2 = erorr\n",
        "      grad_W2 = out1.T @ erorr\n",
        "\n",
        "      # layer 1\n",
        "      erorr = erorr @ W2.T * out1 * (1 - out1)\n",
        "      grad_B1 = erorr\n",
        "      grad_W1 = x @ erorr\n",
        "\n",
        "      # update\n",
        "\n",
        "      # layer 1\n",
        "      W1 -= Î· * grad_W1\n",
        "      B1 -= Î· * grad_B1\n",
        "\n",
        "      # layer 2\n",
        "      W2 -= Î· * grad_W2\n",
        "      B2 -= Î· * grad_B2\n",
        "\n",
        "      # layer 3\n",
        "      W3 -= Î· * grad_W3\n",
        "      B3 -= Î· * grad_B3\n",
        "\n",
        "  # test\n",
        "\n",
        "  Y_pred_test = []\n",
        "  for x, y in zip(X_test, Y_test):\n",
        "\n",
        "      x = x.reshape(-1, 1)\n",
        "\n",
        "      # forward\n",
        "\n",
        "      # layer 1\n",
        "      out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "      # layer 2\n",
        "      out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "      # layer 3\n",
        "      y_pred = softmax(out2 @ W3 + B3)\n",
        "\n",
        "      Y_pred_test.append(y_pred)\n",
        "\n",
        "\n",
        "  Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
        "  loss_train = root_mean_squired_error(Y_train, Y_pred_train)\n",
        "  accuracy_train = np.sum(np.argmax(Y_train, axis=1) == np.argmax(Y_pred_train, axis=1)) / len(Y_train)\n",
        "  print('loss train : ',loss_train)\n",
        "  print('accuracy train : ',accuracy_train)\n",
        "\n",
        "  Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
        "  loss_test = root_mean_squired_error(Y_test, Y_pred_test)\n",
        "  accuracy_test = np.sum(np.argmax(Y_test, axis=1) == np.argmax(Y_pred_test, axis=1)) / len(Y_test)\n",
        "  print('loss test : ',loss_test)\n",
        "  print('accuracy test : ',accuracy_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV0UFuUgqrOL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
