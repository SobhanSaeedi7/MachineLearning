{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Implement a simple neural network (Multi Layer Perceptron) on Scikit-Learn digits dataset**\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437, 10), (360, 10))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_digits()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "Y = np.eye(10)[Y] # one hot\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "  return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "  return np.exp(X) / np.sum(np.exp(X))\n",
    "\n",
    "def root_mean_squired_error(Y_gt, Y_pred):\n",
    "  return np.sqrt(np.mean((Y_gt - Y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "η = 0.001 # learning rate\n",
    "\n",
    "D_in = X_train.shape[1]\n",
    "H1 = 128\n",
    "H2 = 32\n",
    "D_out = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(D_in, H1)\n",
    "W2 = np.random.randn(H1, H2)\n",
    "W3 = np.random.randn(H2, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = np.random.randn(1, H1)\n",
    "B2 = np.random.randn(1, H2)\n",
    "B3 = np.random.randn(1, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  Y_pred_train = []\n",
    "\n",
    "  # train\n",
    "\n",
    "  for x, y in zip(X_train, Y_train):\n",
    "\n",
    "      x = x.reshape(-1, 1)\n",
    "\n",
    "      # forward\n",
    "\n",
    "      # layer 1\n",
    "      out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "      # layer 2\n",
    "      out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "      # layer 3\n",
    "      y_pred = softmax(out2 @ W3 + B3)\n",
    "\n",
    "      Y_pred_train.append(y_pred)\n",
    "\n",
    "      # backward\n",
    "\n",
    "      # layer 3\n",
    "      erorr = -2 * (y - y_pred)\n",
    "      grad_B3 = erorr\n",
    "      grad_W3 = out2.T @ erorr\n",
    "\n",
    "      # layer 2\n",
    "      erorr = erorr @ W3.T * out2 * (1 - out2)\n",
    "      grad_B2 = erorr\n",
    "      grad_W2 = out1.T @ erorr\n",
    "\n",
    "      # layer 1\n",
    "      erorr = erorr @ W2.T * out1 * (1 - out1)\n",
    "      grad_B1 = erorr\n",
    "      grad_W1 = x @ erorr\n",
    "\n",
    "      # update\n",
    "\n",
    "      # layer 1\n",
    "      W1 -= η * grad_W1\n",
    "      B1 -= η * grad_B1\n",
    "\n",
    "      # layer 2\n",
    "      W2 -= η * grad_W2\n",
    "      B2 -= η * grad_B2\n",
    "\n",
    "      # layer 3\n",
    "      W3 -= η * grad_W3\n",
    "      B3 -= η * grad_B3\n",
    "\n",
    "  # test\n",
    "\n",
    "  Y_pred_test = []\n",
    "  for x, y in zip(X_test, Y_test):\n",
    "\n",
    "      x = x.reshape(-1, 1)\n",
    "\n",
    "      # forward\n",
    "\n",
    "      # layer 1\n",
    "      out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "      # layer 2\n",
    "      out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "      # layer 3\n",
    "      y_pred = softmax(out2 @ W3 + B3)\n",
    "\n",
    "      Y_pred_test.append(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train :  0.058154717522257156\n",
      "accuracy train :  0.9846903270702854\n",
      "loss test :  0.13935409191973178\n",
      "accuracy test :  0.8722222222222222\n"
     ]
    }
   ],
   "source": [
    "  Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
    "  loss_train = root_mean_squired_error(Y_train, Y_pred_train)\n",
    "  accuracy_train = np.mean(np.argmax(Y_train, axis=1) == np.argmax(Y_pred_train, axis=1))\n",
    "  print('loss train : ',loss_train)\n",
    "  print('accuracy train : ',accuracy_train)\n",
    "\n",
    "  Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
    "  loss_test = root_mean_squired_error(Y_test, Y_pred_test)\n",
    "  accuracy_test = np.mean(np.argmax(Y_test, axis=1) == np.argmax(Y_pred_test, axis=1))\n",
    "  print('loss test : ',loss_test)\n",
    "  print('accuracy test : ',accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Implement your neural network as a class (Object Oriented Programming)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, Input_length, Hidden_Layer1, Function_Hidden_Layer1, Hidden_Layer2, Function_Hidden_Layer2, Output_Length, Function_Output, epochs, η):\n",
    "        self.epochs = epochs\n",
    "        self.η = η\n",
    "\n",
    "        self.D_in = Input_length\n",
    "        self.H1 = Hidden_Layer1\n",
    "        self.H2 = Hidden_Layer2\n",
    "        self.D_out = Output_Length\n",
    "\n",
    "        self.func_H1 = Function_Hidden_Layer1\n",
    "        self.func_H2 = Function_Hidden_Layer2\n",
    "        self.func_out = Function_Output\n",
    "\n",
    "        self.W1 = np.random.randn(D_in, H1)\n",
    "        self.W2 = np.random.randn(H1, H2)\n",
    "        self.W3 = np.random.randn(H2, D_out)\n",
    "\n",
    "        self.B1 = np.random.randn(1, H1)\n",
    "        self.B2 = np.random.randn(1, H2)\n",
    "        self.B3 = np.random.randn(1, D_out)\n",
    "\n",
    "        self.Y_pred_train = []\n",
    "        self.Y_pred_test = []\n",
    "\n",
    "        self.accuracies = []\n",
    "        self.losses = []\n",
    "\n",
    "    def activation(self, function, x):\n",
    "        if function == \"sigmoid\":\n",
    "            return 1/(1 + np.exp(-x))\n",
    "        \n",
    "        if function == \"softmax\":\n",
    "            return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.Y_pred_train = []\n",
    "\n",
    "            for x, y in zip(X_train, Y_train):\n",
    "                x = x.reshape(-1, 1)\n",
    "\n",
    "                # forward\n",
    "\n",
    "                # layer 1\n",
    "                out1 = self.activation(self.func_H1, x.T @ self.W1 + self.B1)\n",
    "\n",
    "                # layer 2\n",
    "                out2 = self.activation(self.func_H2, out1 @ self.W2 + self.B2)\n",
    "\n",
    "                # layer 3\n",
    "                y_pred = self.activation(self.func_out, out2 @ self.W3 + self.B3)\n",
    "\n",
    "                self.Y_pred_train.append(y_pred)\n",
    "\n",
    "                # backward\n",
    "\n",
    "                # layer 3\n",
    "                erorr = -2 * (y - y_pred)\n",
    "                grad_B3 = erorr\n",
    "                grad_W3 = out2.T @ erorr\n",
    "\n",
    "                # layer 2\n",
    "                erorr = erorr @ self.W3.T * out2 * (1 - out2)\n",
    "                grad_B2 = erorr\n",
    "                grad_W2 = out1.T @ erorr\n",
    "\n",
    "                # layer 1\n",
    "                erorr = erorr @ self.W2.T * out1 * (1 - out1)\n",
    "                grad_B1 = erorr\n",
    "                grad_W1 = x @ erorr\n",
    "\n",
    "                # update\n",
    "\n",
    "                # layer 1\n",
    "                self.W1 -= self.η * grad_W1\n",
    "                self.B1 -= self.η * grad_B1\n",
    "\n",
    "                # layer 2\n",
    "                self.W2 -= self.η * grad_W2\n",
    "                self.B2 -= self.η * grad_B2\n",
    "\n",
    "                # layer 3\n",
    "                self.W3 -= self.η * grad_W3\n",
    "                self.B3 -= self.η * grad_B3\n",
    "            loss, accuracy = self.evaluate(X_train, Y_train)\n",
    "\n",
    "            self.accuracies.append(accuracy)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        self.Y_pred_test = []\n",
    "        for x in X_test:\n",
    "            \n",
    "            x = x.reshape(-1,1)\n",
    "\n",
    "            # forward\n",
    "\n",
    "            # layer 1\n",
    "            out1 = self.activation(self.func_H1, x.T @ self.W1 + self.B1)\n",
    "\n",
    "            # layer 2\n",
    "            out2 = self.activation(self.func_H2, out1 @ self.W2 + self.B2)\n",
    "\n",
    "            # layer 3\n",
    "            y_pred = self.activation(self.func_out, out2 @ self.W3 + self.B3)\n",
    "\n",
    "            self.Y_pred_test.append(y_pred)\n",
    "            return self.Y_pred_test\n",
    "\n",
    "    def calculate_loss(self, Y_pred, Y_test, metric='mse'):\n",
    "        if metric == 'mse':\n",
    "            loss = np.mean((Y_pred - Y_test) ** 2)\n",
    "\n",
    "        elif metric == 'mae':\n",
    "            loss = np.mean(np.abs(Y_pred - Y_test))\n",
    "\n",
    "        elif metric == 'rmse':\n",
    "            loss = np.sqrt(np.mean((Y_pred - Y_test) ** 2))\n",
    "        else:\n",
    "            raise Exception('Metric not found!')\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, Y_pred, Y_test):\n",
    "        accuracy = np.mean(np.argmax(Y_train, axis=1) == np.argmax(Y_pred_train, axis=1))\n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        Y_pred = self.predict(X_test)\n",
    "        loss = self.calculate_loss(Y_pred, Y_test)\n",
    "        accuracy = self.calculate_accuracy(Y_pred, Y_test)\n",
    "        return loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Implement fit, evaluate and predict methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(D_in, H1, 'sigmoid', H2, 'sigmoid', D_out, 'softmax', epochs, η)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1349398680093473, 0.9846903270702854)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_test[77] \n",
    "np.argmax(Y_test, axis=1)[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict([x]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Check your predict method with images of the numbers you wrote in your own handwriting\n",
    "\n",
    "VERY IMPORTANT!!!! Report your loss and accuracy on train and test data in readme.md\n",
    "\n",
    "Plot loss and accuracy using MatPlotLib\n",
    "\n",
    "Implement OneHotEncoder and OneHotDecoder functions from scratch, Then compare your function with Scikit-Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
